---
title: "Capstone Rough Draft"
author: "Avery Pike"
format: html
server: shiny
---

## Shiny Documents

This Quarto document is made interactive using Shiny. Interactive documents allow readers to modify parameters and see the results immediately. Learn more about Shiny interactive documents at <https://quarto.org/docs/interactive/shiny/>.

## Inputs and Outputs

You can embed Shiny inputs and outputs in your document. Outputs are automatically updated whenever inputs change. This demonstrates how a standard R plot can be made interactive:

```{r}
sliderInput("bins", "Number of bins:", 
            min = 1, max = 50, value = 30)
plotOutput("distPlot")
```

```{r}
#| context: server
output$distPlot <- renderPlot({
   x <- faithful[, 2]  # Old Faithful Geyser data
   bins <- seq(min(x), max(x), length.out = input$bins + 1)
   hist(x, breaks = bins, col = 'darkgray', border = 'white',
        xlab = 'Waiting time to next eruption (in mins)',
        main = 'Histogram of waiting times')
})
```


# Abstract
This study explores a novel methodology for analyzing X-ray fluorescence (XRF) data from the Ness of Brodgar archaeological site using machine learning techniques. XRF has been instrumental in identifying elemental compositions, but its potential for developing comparative geochemical signatures within large datasets remains underutilized. The project addresses this by applying Principal Component Analysis (PCA) to simplify complex spectral data into distinctive geochemical signatures, facilitating site-specific comparisons. Instead of producing universally quantifiable data, the approach emphasizes relative comparisons within the site, enhancing interpretative potential for understanding patterns and relationships. By transforming the extensive geochemical database into simplified signatures, we enable more detailed analyses of site-specific variations and connections.

The methodologies and tools developed serve as a blueprint for similar projects, offering a framework that can be adapted to other archaeological sites with extensive geochemical data. Although tailored to the Ness of Brodgar, this approach can enrich interpretations of geochemical signatures across different contexts. The project successfully demonstrates how PCA can simplify and enhance the analysis of complex XRF spectral data. This approach not only improves the ability to interpret site-specific patterns but also sets a precedent for integrating geochemical data with archaeological research, advancing the fieldâ€™s analytical capabilities.

# Introduction
X-ray fluorescent spectrometry (XRF) has been a cornerstone technique in archaeological science for many years. Its applications have ranged from identifying the elemental composition of artifacts to sourcing raw materials and understanding ancient manufacturing processes. However, XRF's potential for developing specific geochemical signatures within large archaeological databases remains largely untapped. This study seeks to bridge this gap by demonstrating a novel methodology using machine learning algorithms and statistical insights on data derived from XRF to create comparative geochemical signatures within a localized archaeological database.

Using thousands of geochemical samples collected from the Ness of Brodgar archaeological site, this project analyzes a comprehensive spectral database that allows for site-specific internal comparisons. Unlike traditional approaches that provide universally quantifiable data in parts per million (ppm), this study emphasizes relative comparison within the site-specific context. By employing Principal Component Analysis (PCA), we transform complex spectral data into simplified, distinctive signatures that are easy to compare across the larger dataset.

The innovative methodology presented in this project provides a new level of geochemical comparative data, enabling archaeological researchers to make more informed interpretations about site-specific patterns and relationships. This localized approach, while not offering universally quantifiable results, significantly enhances the ability to compare samples within the greater collection of data from the Ness of Brodgar, opening new avenues for archaeological research and analysis. The tools derived from this project will help researchers provide scientific foundations for future publications and interpretations by integrating their own studies with the relevant geochemical data from the site.

# Background Information
## The Ness of Brodgar
The Ness of Brodgar is a Neolithic archaeological complex located in the Orkney Islands, Scotland, UK. Situated in the heart of Neolithic Orkney, a UNESCO World Heritage site, the Ness has been pivotal in understanding Neolithic civilization in the North Atlantic. The complex features massive stone structures that span the width of the isthmus where the Ness is located. The site was in regular use for hundreds of years, with evidence of occupation dating back to at least 3300 BCE and extending over a thousand years later prior to its abandonment. This historical significance as well as the massive scale of the excavation itself makes the Ness of Brodgar one of the most important archaeological excavations in the world.

Figure 1a: An aerial image of Trench P taken in (YEAR NEEDED).
```{r}

```

Figure 1b: A diagram of Structure 8 located in Trench P.
```{r}

```

Due to the prehistoric nature of Neolithic archaeology, interpretations of material culture heavily rely on scientific studies. As one of the largest active archaeological sites globally, the Ness has attracted numerous archaeologists eager to utilize modern technology and innovative techniques to extract as much information as possible from the material culture. One such project involves the regular geochemical sampling of soils within and around the excavated structures on the site. Over twelve dig seasons, excavators have collected soil samples for geochemical analysis at regular intervals. These analyses, conducted using X-ray fluorescent spectroscopy (XRF), have resulted in one of the largest databases of geochemical spectral signatures in archaeological research.

## X-ray Fluorescent Spectroscopy (XRF)
XRF spectroscopy is an analytical technique used to determine the elemental composition of materials. The process begins by directing high-energy X-rays at a sample, causing the atoms within the material to become excited and emit secondary (or fluorescent) X-rays. Each element in the material emits X-rays at specific characteristic energies, allowing for their identification and quantification. Specifically, the X-rays cause electrons in the inner shells of the atoms to be ejected. As electrons from higher energy levels fall into the lower energy levels to fill these vacancies, they emit X-rays with energies that are characteristic of the elements present in the sample.

The output of XRF analysis is a spectrum that displays the energy levels of the emitted X-rays, with peaks corresponding to different elements. The intensity of these peaks can be used to determine the concentration of each element within the sample. This spectrum provides a comprehensive elemental fingerprint of the material, which can be used to look for various types of analysis.

In archaeological contexts, in this study with soil samples, XRF allows researchers to obtain detailed geochemical profiles of the sampled materials. These profiles can reveal information about the provenance of materials, past human activities, and environmental conditions at the site. The ability to analyze a wide range of elements simultaneously makes XRF a powerful tool for constructing detailed geochemical maps and databases.

Previous archealogical studies have primarily used much smaller quantities of data similar to this study for univariate analysis on specific elemental signatures. At the Ness of Brodgar, early studies have used these techniques in projects that mainly involved creating heatmaps to show the concentration of individual elements within specific structures. While these heatmaps provide valuable insights, they represent a limited scope of what can be achieved with the comprehensive geochemical data available. This project seeks to expand on prior work by employing a novel methodology that leverages the extensive XRF data and the use of the machine learning algorithms (primarily principal component analysis) to create comparative geochemical signatures within the site's database. This approach aims to enhance the interpretative potential of the data, offering a new level of geochemical comparative analysis that has not been previously utilized in archaeological research.

Figure 2: A heatmap created by Lara Shinsato (CITATION NEEDED) showing (COMPLETE INFORMATION):
```{r}

```


# Data Engineering on the Ness of Brodgar Geochemical Database
## Cleaning and Preprocessing
One of the major challenges in processing data at the Ness Of Brodgar is the massive scale of the excavation. The Ness of Brodgar Excavations operated from 2004 - 2024 with hundreds of different excavators present on site over the time period. During this time different projects, data collection systems, and technologies have been used to collect data on the site . The geochemical sampling of soil on the site did not start until 2011, and the techniques necessary to develop and calibrate results consistent enough to produce trustworthy spectral data were not adequate for processing until 2015. Additionally, the plurality of collection systems has led to extremely cumbersome integration between the tens of structures within the site. The data will eventually all be centralized to geospatial locations, allowing for studies to access all of the collected data, but this will be a collaborative process that will take place over the study seasons in upcoming years. Fortunately, Structure 8 has become a prototype for this site wide integration. As such, the data that was needed to provide context for the geochemical data from Structure 8 was in a state that the necessary information for statistical analysis could be used after cleaning the data. In theory, upon completion, the statistical computation systems built for this project will be easily repeatable once collaborators responsible for the collection and wrangling complete their initial data processing and neccessary research.

The data that was available for this project still needed considerable processing prior to conducting any statistical analysis. One of the difficulties when collecting data with an X-ray fluorescent spectrometer (XRF) is that the spectral data captured is not discrete. As such, a software program called Artax, created by Bruker, uses several deconvolution algorithms to differentiate between the respective elemental peaks produced from the sample.

Figure 2: An image of the spectral signatures received from two contexts (roughly 20 visualize tests) can be seen below in figure 1.
```{r}

```

Artax was configured to produce a csv file containing numerical values for every element that produced a peak in the sample. However, these values are not normalized after deconvolution so the values are relatively meaningless. Prior studies have shown that normalizing these values to the Compton peak (recorded as the Rhodium peaks in the Artax produced csv) can address this problem (COMPONT PEAK CITATIONS NEEDED HERE). In the preprocessing stage of this study, this method was used to create data that, while quantitative, only provides relational information. It is at this point that most research in the field has then used established standards taken from a sample with with known quantitative measurements to produce fully quantifiable data with units in parts per million. Due to the nature of the methodology used for data collection at the Ness of Brodgar, no known standards were available to produce ppm quantities. Later statistical analyses in the study show that for most archaeological questions of interest the ppm information is not necessary to inform archaeological interpretation as the standardization to the Compton peak and use of appropriate statistical methods and interpretation is sufficient for localized comparative analyses with a large enough dataset. To improve accuracy of each sample tested by the XRF, 2 or 3 tests were run to ensure consistency within the sample. Third tests were only ran if visual comparison of the two tests showed discrepencies. While not a perfect method, the mean of these test values were taken for every sample in an attempt to improve accuracy.

## Data Structure
Once in a clean state, the data from the geochemical analysis needed to be joined with the relevant data from the site that was not part of the geochemical studies. As mentioned earlier data collected by other specialists will not be available until comprehensive processing during the upcoming study seasons. However, the structural components needed to create relevant geochemical information were made consistent and obtained from structure 8. This information, will be used to look for relationships between geochemical insights to other sources of data throughout the site as research progresses in the upcoming study seasons.

Somewhat confusingly, one of the labels given to geochemical samples during colletion was the "Sample Number", but there are often multiple samples collected for each sample number as some supervisors chose to use the same sample number for samples taken within the same context with different squares. For that reason even though sample is often the correct word to use for an observation in most tables, since sample numbers aren't always unique in this data structure section each sample be referred to as a test for this section only, and sample number will be used to refer to the recorded sample number to avoid confusion. Each test possesses a sample number and a square. The combination of this sample number and square create a compound key for each test. The square number refers to a square on a 50cm x 50cm xy grid, and the sample number can be used in conjunction with the square to find the exact xyz point (in a pointcloud created using GIS software) that the sample was taken from using laser scanning survey technology. It is worth noting that some tests were collected from outside of the grid, and the spatial information from these tests is still recorded. These squareless tests, are marked with a square number of zero and have unique sample numbers. Additionally, each test possesses a context number. Contexts are excavator labelled layers of soil that possess recognizable distinction from other layers of soil. The techniques for identifying different contexts rely on domain expertise and with proper interpretation the tools developed by this project will help to support or possibly correct the manually labelled data. Despite using a numbering system, all of the aforementioned dimensions for a single test can be viewed as non-numerical vectors for the purpose of this geochemical study. The numbering serves a logical purpose for sitewide record keeping, but for geochemical analysis the numbering is arbitrary and is best treated as a non-numerical value.

Ultimately, despite the somewhat convoluted system of test labeling caused by years of individual record keeping systems, the actual structure of the clean geochemical data was relatively straightforward. Figure 3 shows the organization of this data. An extensive amount of cleaning was required to insure that every recorded test was properly collected and processed, but this mostly required examining the datasets then finding and correcting inconsistencies in the record keeping.

Figure 3: An ER diagram of the cleaned data. Most chunks in this file import joined versions of this data to condense code, however, the foundational organization of the data is visualized in the diagram.
```{r}

```

# Data Analysis
## Principal Component Analysis
In trying to conceptualize a geochemical fingerprint fit for comparison accross many different geochemical samples, Machine Learning provides a valuable tool in the use of Principal Component Analysis (PCA). PCA is a dimensionality reduction technique that transforms a dataset with potentially correlated variables into a set of linearly uncorrelated variables known as principal components. The primary objective of PCA is to identify patterns in data and express the data in such a way that the similarities and differences are highlighted. Conversely, the XRF spectra provides readings for many different elemental signatures and as such it can be hard to compare general geochemical makeup across samples. For this reason and due to the limitation of much smaller datasets, most XRF studies have focused on signatures of only a few elements. The size of Ness of Brodgar geochemical database provides a unique opportunity to apply PCA to these signatures.

```{r}
# #| fig-cap: PCA Biplot of the Normalized Geochemical Data
# #| fig-alt: PCA Biplot showing the relationship between samples and principal components
# #| out-width: 100%
# #| out-height: 600px
# library(ggplot2)
# 
# # Load the dataset
# standardized_data <- read.csv("StandardizedData.csv")
# 
# # Perform PCA
# pca_result <- prcomp(standardized_data, scale. = TRUE)
# pca_components <- as.data.frame(predict(pca_result))
# pca_components_df <- cbind(non_numeric_cols, pca_components)
# 
# 
# # Biplot
# biplot(pca_result, main = "PCA Biplot of the Normalized Data")
```


### Dimension Reduction for Developing Geochemical Fingerprint
PCA accomplishes a number of tasks for the purpose of this study. The first of which is that conducting principal component analysis is able to reduce the many dimensions of elemental signatures present in a geochemical sample into Principal Components that capture the variance of the dataset while reducing the dimensions. Future research at the Ness of Brodgar will look to understand the relationship between geochemical composition and other data accross the site. While specific elemental signatures will sometimes be of interest, more often than not having a general understanding of the entire geochemical signature of a soil relative to other areas of the site will hold more value to the researcher. To allow for such comparisons, PCA can be used for dimension reduction. After normalizing the dimensions with a standard scaling function made possible by the gaussian distribution of the individual elemental dimensions, PCA machine learning algorithms were run on the dataset. By plotting the values of Principal Components 1 and 2 for each sample a general understanding of similarity accross samples could be visualized.

### Understanding Elemental Influence
PCA also helped to address a secondary question regarding the geochemical makeup of the samples. Geochemical specialists expressed interest in knowing which specific elements contributed the most to the variance of the overall signature on the site. One of the added benefits of conducting PCA is that it provides loadings, which indicate the contribution of each original variable (element) to the principal components.

In this study, the loadings for Principal Components 1 and 2 were calculated and tabulated. These loadings reveal which elements have the greatest influence on the variance captured by these principal components. Specifically, the elements with the highest absolute loadings for PC1 are the ones that affect the overall variance of the data the most. Similarly, the elements with high absolute loadings for PC2 are influential in the second direction of variance.

By examining these loadings, researchers can identify which elements are driving the major patterns observed in the data. For example, if an element such as Iron (Fe) has a high loading on PC1, it suggests that variations in Iron concentration are a significant source of variability in the dataset. This information can guide further geochemical analysis and help in understanding the elemental composition of different contexts within the site.


Figure: A table showing the loading values (coefficients of the eigenvalues) for Principal Component 1 and 2 of PCA on Structure 8.
```{r}
# #| echo: false
# #| results: hide
# pca_loadings <- data.frame(PC1 = pca_result$rotation[,1], 
#                             PC2 = pca_result$rotation[,2], 
#                             Element = colnames(data[, -1]))
# 
# # Save the PCA loadings table as CSV for reference
# # write.csv(pca_loadings, "data/pca_loadings.csv", row.names = FALSE)
# 
# # Display the PCA loadings table
# pca_loadings
```


### Interactive Principal Component Plot for Visualization
An interactive principal component plot was developed to aid researchers and specialists in visualizing the geochemical data. This tool allows users to select specific contexts or phases from a dropdown menu and observe the relationships and similarities between different areas of the site. By plotting the principal components, users can intuitively grasp the underlying geochemical patterns and how different contexts compare to one another.

The interactive nature of the plot makes it possible to highlight specific samples and contexts, facilitating a deeper understanding of the spatial distribution of geochemical signatures. This visualization tool is designed to be user-friendly, allowing site specialists to explore the data dynamically and make informed interpretations based on the visual representations of the principal components.

The dropdown selection feature enhances the utility of the PCA results by enabling targeted analysis. Researchers can focus on particular areas of interest, compare them to other parts of the site, and potentially identify geochemical trends that might have been overlooked in a static analysis. This interactive approach significantly improves the ability to conduct detailed geochemical investigations and supports the ongoing research efforts at the Ness of Brodgar.

```{r}
library(shiny)

# Run the Shiny app
shiny::runApp("app.R", launch.browser = FALSE)
```


# Ethical Considerations
### Ethical Considerations
In conducting this research, several ethical considerations were taken into account to ensure the integrity and responsible handling of the data and findings.

Data privacy and ownership were of utmost importance. Proper permissions were obtained for the use of the data, and its ownership was respected throughout the study. Any proprietary information was handled with confidentiality, ensuring that the data owners retained control over their contributions. The cultural sensitivity associated with the Ness of Brodgar was acknowledged. The site holds cultural and historical significance, and the research was conducted with an awareness of preserving this heritage. Findings were presented in a manner that respects the historical context and the value of the site to the local community and the broader archaeological field. Accuracy and transparency were prioritized. The methodologies and analytical techniques used were rigorously tested and validated. Transparency in reporting the methods and results was maintained to allow for reproducibility and verification by other researchers. Environmental impact was also considered. The geochemical sampling and subsequent analyses were conducted with minimal disruption to the site. Efforts were made to ensure that the sampling process did not disturb the archaeological context or the surrounding environment.

By addressing these ethical considerations, the study aims to contribute valuable insights while maintaining the highest standards of research integrity and respect for the archaeological site and its stakeholders.

# Conclusion
The innovative methodology developed in this study offers a valuable tool for archaeological specialists aiming to uncover relationships between geochemical data and other forms of archaeological evidence. By employing Principal Component Analysis (PCA) to reduce the complexity of geochemical signatures, researchers can now compare simplified, distinctive signatures across different contexts within the site. This approach provides a new level of geochemical comparative data, enabling more informed interpretations of site-specific patterns and relationships. While this study focuses on the Ness of Brodgar, the methodologies and tools developed here serve as a blueprint for similar projects. Other archaeological sites with extensive geochemical data can benefit from the PCA approach to gain insights into their data. The ability to identify which elements contribute most significantly to the variance in geochemical data allows for targeted analysis and more nuanced interpretations, aiding in understanding trade patterns, site formation processes, and human activities.

The study's success in using PCA to analyze the Ness of Brodgar's geochemical data highlights its potential for broader applications. By transforming complex spectral data into accessible and interpretable signatures, researchers can more easily compare samples and identify significant geochemical patterns. This localized comparative approach enhances the interpretative potential of geochemical data, providing a robust framework for future archaeological research using data from the site. The primary limitation of the project is that the study's approach is tailored to the specific site conditions at the Ness of Brodgar caused by a lack of known geochemical standards. While the methodology can serve as a blueprint, any given archaeological site will present unique challenges and geochemical characteristics that require adaptation. Nonetheless, the ability to integrate geochemical data with other archaeological evidence opens new avenues for detailed and contextually relevant interpretations. The insights produced from data analysis in this project are valid, but must be acknowledged as site specific. 

In conclusion, the methodology presented in this project successfully demonstrates the potential of machine learning techniques, specifically PCA, in transforming complex geochemical data into simplified signatures that are easy to compare across larger datasets. This approach enhances the ability to interpret site-specific patterns and relationships, ultimately contributing to the advancement of archaeological science. The tools derived from this project will help researchers provide scientific foundations for future publications and interpretations by integrating their own studies with relevant geochemical data from the site.

# Sections to Add?

## Clustering of Contexts
(This area has been explored, but has not drawn interest to archaeologists on the site. They do not believe that ML clustering algorithms derived solely from the geochemical makeup is more valuable than )

## Visualizing Specific Elemental Differences
Just needs to be done. Relatively simple with limited statistical analysis. Will be another interactive visualization that allows for selection of elements.

## Developing Similarity Scores
Added statistical element that I will look to explore if I have time.
