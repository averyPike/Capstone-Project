# Data
## Cleaning and Preprocessing
One of the major challenges in processing data at the Ness Of Brodgar is the massive scale of the excavation. The Ness of Brodgar Excavations operated from 2004 - 2024 with hundreds of different excavators present on site over the time period. During this time different projects, data collection systems, and technologies have been used to collect data on the site . The geochemical sampling of soil on the site did not start until 2011, and the techniques necessary to develop and calibrate results consistent enough to produce trustworthy spectral data were not adequate for processing until 2015. Additionally, the plurality of collection systems has led to extremely cumbersome integration between the tens of structures within the site. The data will eventually all be centralized to geospatial locations, allowing for studies to access all of the collected data, but this will be a collaborative process that will take place over the study seasons in upcoming years. Fortunately, Structure 8 has become a prototype for this site wide integration. As such, the data that was needed to provide context for the geochemical data from Structure 8 was in a state that the necessary information for statistical analysis could be used after cleaning the data. In theory, upon completion, the statistical computation systems built for this project will be easily repeatable once collaborators responsible for the collection and wrangling complete their initial data processing and neccessary research.

The data that was available for this project still needed considerable processing prior to conducting any statistical analysis. One of the difficulties when collecting data with an X-ray fluorescent spectrometer (XRF) is that the spectral data captured is not discrete. As such, a software program called Artax, created by Bruker, uses several deconvolution algorithms to differentiate between the respective elemental peaks produced from the sample.

Figure 2: An image of the spectral signatures received from two contexts (roughly 20 visualize tests) can be seen below in figure 1.
```{r}

```

Artax was configured to produce a csv file containing numerical values for every element that produced a peak in the sample. However, these values are not normalized after deconvolution so the values are relatively meaningless. Prior studies have shown that normalizing these values to the Compton peak (recorded as the Rhodium peaks in the Artax produced csv) can address this problem (COMPONT PEAK CITATIONS NEEDED HERE). In the preprocessing stage of this study, this method was used to create data that, while quantitative, only provides relational information. It is at this point that most research in the field has then used established standards taken from a sample with with known quantitative measurements to produce fully quantifiable data with units in parts per million. Due to the nature of the methodology used for data collection at the Ness of Brodgar, no known standards were available to produce ppm quantities. Later statistical analyses in the study show that for most archaeological questions of interest the ppm information is not necessary to inform archaeological interpretation as the standardization to the Compton peak and use of appropriate statistical methods and interpretation is sufficient for localized comparative analyses with a large enough dataset. To improve accuracy of each sample tested by the XRF, 2 or 3 tests were run to ensure consistency within the sample. Third tests were only ran if visual comparison of the two tests showed discrepencies. While not a perfect method, the mean of these test values were taken for every sample in an attempt to improve accuracy.

## Data Structure
Once in a clean state, the data from the geochemical analysis needed to be joined with the relevant data from the site that was not part of the geochemical studies. As mentioned earlier data collected by other specialists will not be available until comprehensive processing during the upcoming study seasons. However, the structural components needed to create relevant geochemical information were made consistent and obtained from structure 8. This information, will be used to look for relationships between geochemical insights to other sources of data throughout the site as research progresses in the upcoming study seasons.

Somewhat confusingly, one of the labels given to geochemical samples during colletion was the "Sample Number", but there are often multiple samples collected for each sample number as some supervisors chose to use the same sample number for samples taken within the same context with different squares. For that reason even though sample is often the correct word to use for an observation in most tables, since sample numbers aren't always unique in this data structure section each sample be referred to as a test for this section only, and sample number will be used to refer to the recorded sample number to avoid confusion. Each test possesses a sample number and a square. The combination of this sample number and square create a compound key for each test. The square number refers to a square on a 50cm x 50cm xy grid, and the sample number can be used in conjunction with the square to find the exact xyz point (in a pointcloud created using GIS software) that the sample was taken from using laser scanning survey technology. It is worth noting that some tests were collected from outside of the grid, and the spatial information from these tests is still recorded. These squareless tests, are marked with a square number of zero and have unique sample numbers. Additionally, each test possesses a context number. Contexts are excavator labelled layers of soil that possess recognizable distinction from other layers of soil. The techniques for identifying different contexts rely on domain expertise and with proper interpretation the tools developed by this project will help to support or possibly correct the manually labelled data. Despite using a numbering system, all of the aforementioned dimensions for a single test can be viewed as non-numerical vectors for the purpose of this geochemical study. The numbering serves a logical purpose for sitewide record keeping, but for geochemical analysis the numbering is arbitrary and is best treated as a non-numerical value.

Ultimately, despite the somewhat convoluted system of test labeling caused by years of individual record keeping systems, the actual structure of the clean geochemical data was relatively straightforward. Figure 3 shows the organization of this data. An extensive amount of cleaning was required to insure that every recorded test was properly collected and processed, but this mostly required examining the datasets then finding and correcting inconsistencies in the record keeping.

Figure 3: An ER diagram of the cleaned data. Most chunks in this file import joined versions of this data to condense code, however, the foundational organization of the data is visualized in the diagram.
```{r}

```
